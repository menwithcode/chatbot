{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f806be0b-f89f-4224-a448-54dadb3e4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import webbrowser\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47f97977-46a9-487e-a4e1-0a0a58c410b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def google_search(query, custom_search_engine_id, api_key):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&cx={custom_search_engine_id}&key={api_key}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        links = []  # List to store all the links\n",
    "        if 'items' in data:\n",
    "            for item in data['items']:\n",
    "                title = item.get('title')\n",
    "                link = item.get('link')\n",
    "                # print(f\"Title: {title}\")\n",
    "                # print(f\"Link: {link}\\n\")\n",
    "                links.append(link)  # Add the link to the list\n",
    "            return links# Return the list of links  # Return the link of the first search result\n",
    "        else:\n",
    "            print(\"No search results found.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    return links\n",
    "def scrap_content(url):\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return content\n",
    "\n",
    "def save_to_excel(content, filename):\n",
    "    # Create a DataFrame from the content\n",
    "    df = pd.DataFrame(content, columns=['Contents'])\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df.to_excel(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "715ab431-86db-472e-9e61-ae5234b524d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter something:  tell me the case of atul subhash sucide diet?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/LegalAdviceIndia/comments/1heums6/what_alternative_steps_could_atul_subhash_have/', 'https://m.economictimes.com/magazines/panache/this-delhi-restaurant-is-paying-homage-to-atul-subhashs-tragic-death-with-every-meal-check-the-viral-tribute/articleshow/116400675.cms', 'https://www.reddit.com/r/india/comments/1hb0exa/bengaluru_techie_atul_subhashs_shattering_suicide/', 'https://www.instagram.com/docrahulchawla/p/DCtKw4wzpf2/', 'https://www.facebook.com/photo.php?fbid=3358140200896383&id=812991898744572&set=a.813570065353422', 'https://timesofindia.indiatimes.com/city/bengaluru/bengaluru-techie-subhash-atul-suicide-experts-say-dowry-laws-for-womens-safety-used-as-weapon/articleshow/116196509.cms', 'https://www.linkedin.com/posts/anulall_subhash-atuls-tragic-case-is-a-reminder-activity-7272840398299893761-P6nt', 'https://timesofindia.indiatimes.com/city/bengaluru/judge-laughed-when-estranged-wife-told-bengaluru-techie-atul-to-commit-suicide/articleshow/116234749.cms', 'https://www.livemint.com/news/trends/techie-in-bengaluru-dies-by-suicide-leaves-24-page-note-blaming-wife-her-family-justice-is-due-11733817488438.html', 'https://timesofindia.indiatimes.com/city/bengaluru/tragic-suicide-of-executive-allegedly-linked-to-3-crore-demand-from-in-laws-ignites-outrage/articleshow/116184453.cms']\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Please enter something: \")\n",
    "custom_search_engine_id = '8779af79fac0c4471'\n",
    "api_key = 'AIzaSyAKS1kPpyfMk9_3Kp5VVkdrd8d9G5gMKOw'\n",
    "\n",
    "links = google_search(user_input, custom_search_engine_id, api_key)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48ed4c58-fd5e-484c-a8af-864ac9868cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              A place to ask simple legal questions pertaining to India. Advice here is for informational purposes only and should not be considered final or official advice. See a local advocate for the best answer to your questions.                 Hello,I'm interested in knowing how Atul could have defended himself. Given that Indian laws seem to favour women, I'd appreciate any suggestions or opinions on how he could have handled the situation. I'm asking so that people are aware of their options before making drastic decisions.   Anyone can view, post, and comment to this community             ET Year-end Special Reads Take That: The gamechanger weapon's India acquired in 2024 10 big-bang policy moves Modi government made in 2024 How governments tried to rein in the social media beast (Catch all the Business News, Breaking News, Budget 2024 Events and Latest News Updates on The Economic Times.) Subscribe to The Economic Times Prime and read the ET ePaper online. (Catch all the Business News, Breaking News, Budget 2024 Events and Latest News Updates on The Economic Times.) Subscribe to The Economic Times Prime and read the ET ePaper online.  Five reasons why FIIs may continue to sell Indian equities S Naren to Raamdeo Agrawal to Nilesh Shah, what fund managers have been reading in 2024 The AI company that is “killing” it in the stock market and outside of it as well Fed’s “dot plot” signals a change in tone, and why its ripple effect will go beyond markets Can Cipla win over diabetes patients with a painless, inhalable insulin? When Indian ‘body shops’ turn balance of payment bosses Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Prime Articles Top Slideshow Top Definitions Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account  and log in again using your ET Prime credentials  to enjoy all member benefits.  To read full story, subscribe to ET Prime ₹34 per week                     Billed annually at                     ₹2499 ₹1749 Super Saver Sale - Flat 30% Off On ET Prime Membership   Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then  ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then  ₹ 1749 for 1 year Special Offer Get flat 35% off on ETPrime     90 Days Prime access worth Rs999 unlocked for you Exclusive Economic Times Stories, Editorials & Expert opinion across 20+ sectors Stock analysis. Market Research. Industry Trends on 4000+ Stocks ​Get 1 Year Complimentary Subscription of TOI+ worth Rs.799/-​ Stories you might be interested in               The Official Subreddit for India             Anyone can view, post, and comment to this community            \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    all_contents = []\n",
    "    for url in links[0:5]:\n",
    "        content = scrap_content(url)\n",
    "        if content:\n",
    "            # Remove special characters from the content\n",
    "            cleaned_content = re.sub(r\"[\\n]\", \"\", content)\n",
    "            all_contents.append(cleaned_content)  # Append the cleaned content to the list\n",
    "\n",
    "    combined_contents = \" \".join(all_contents)\n",
    "    if len(combined_contents) > 30000:\n",
    "        combined_contents = combined_contents[:30000]\n",
    "    print(combined_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6817b6f-9a75-40c5-9d6e-e9a8ece94519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3438\n"
     ]
    }
   ],
   "source": [
    "user_input = user_input.replace('?', '').replace('.', '')\n",
    "additional_sentence = \" . \" + user_input\n",
    "\n",
    "# Add the sentence to the content\n",
    "content_with_addition = combined_contents + additional_sentence\n",
    "print(len(content_with_addition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "873e4108-95c4-400e-b8f6-80c914ceb94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropout 1 (Dropped sentences [5, 4, 3]):\n",
      "Remaining Indices: [1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "\n",
      "Dropout 2 (Dropped sentences [2, 15, 13]):\n",
      "Remaining Indices: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]\n",
      "\n",
      "Dropout 3 (Dropped sentences [14, 9, 8]):\n",
      "Remaining Indices: [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 14, 15]\n",
      "\n",
      "Dropout 4 (Dropped sentences [10, 3, 16]):\n",
      "Remaining Indices: [1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14]\n",
      "\n",
      "Dropout 5 (Dropped sentences [7, 11, 3]):\n",
      "Remaining Indices: [1, 3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15]\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.05827182 0.        ]\n",
      " [0.         1.         0.08282023 1.         0.         0.00628723\n",
      "  0.         0.04864268 0.19676399 0.10372703 0.         0.        ]\n",
      " [0.         0.08282023 1.         0.08282023 0.         0.07695335\n",
      "  0.         0.         0.         0.01498755 0.         0.        ]\n",
      " [0.         1.         0.08282023 1.         0.         0.00628723\n",
      "  0.         0.04864268 0.19676399 0.10372703 0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.01071254\n",
      "  0.         0.         0.         0.01046298 0.         0.        ]\n",
      " [0.         0.00628723 0.07695335 0.00628723 0.01071254 1.\n",
      "  0.         0.00339612 0.02306602 0.00568884 0.01698339 0.02870451]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.04864268 0.         0.04864268 0.         0.00339612\n",
      "  0.         1.         0.04142844 0.04949869 0.         0.        ]\n",
      " [0.         0.19676399 0.         0.19676399 0.         0.02306602\n",
      "  0.         0.04142844 1.         0.3029285  0.         0.        ]\n",
      " [0.         0.10372703 0.01498755 0.10372703 0.01046298 0.00568884\n",
      "  0.         0.04949869 0.3029285  1.         0.09952642 0.        ]\n",
      " [0.05827182 0.         0.         0.         0.         0.01698339\n",
      "  0.         0.         0.         0.09952642 1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.02870451\n",
      "  0.         0.         0.         0.         0.         1.        ]]\n",
      "[[1.         0.         0.         0.02809348 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.05827182 0.        ]]\n",
      " tell me the case of atul subhash sucide diet Hello,I'm interested in knowing how Atul could have defended himself. When Indian ‘body shops’ turn balance of payment bosses Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Prime Articles Top Slideshow Top Definitions Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive?\n",
      "Sentences with centrality scores greater than 0.00 (sorted in descending order):\n",
      " Anyone can view, post, and comment to this community             ET Year-end Special Reads Take That: The gamechanger weapon's India acquired in 2024 10 big-bang policy moves Modi government made in 2024 How governments tried to rein in the social media beast (Catch all the Business News, Breaking News, Budget 2024 Events and Latest News Updates on The Economic Times.) When Indian ‘body shops’ turn balance of payment bosses Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Prime Articles Top Slideshow Top Definitions Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive? 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then  ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then  ₹ 1749 for 1 year Special Offer Get flat 35% off on ETPrime     90 Days Prime access worth Rs999 unlocked for you Exclusive Economic Times Stories, Editorials & Expert opinion across 20+ sectors Stock analysis. Industry Trends on 4000+ Stocks ​Get 1 Year Complimentary Subscription of TOI+ worth Rs.799/-​ Stories you might be interested in               The Official Subreddit for India             Anyone can view, post, and comment to this community             . Hello,I'm interested in knowing how Atul could have defended himself. Subscribe to The Economic Times Prime and read the ET ePaper online. Subscribe to The Economic Times Prime and read the ET ePaper online. Given that Indian laws seem to favour women, I'd appreciate any suggestions or opinions on how he could have handled the situation. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account  and log in again using your ET Prime credentials  to enjoy all member benefits. To read full story, subscribe to ET Prime ₹34 per week                     Billed annually at                     ₹2499 ₹1749 Super Saver Sale - Flat 30% Off On ET Prime Membership   Offer Exclusively For You Save up to Rs.\n",
      "sigmabot execution time: 0.1167 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sigmabot():\n",
    "    start_time = time.time()  \n",
    "    # Download necessary NLTK resources\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt_tab')\n",
    "    \n",
    "    # Define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # The content provided\n",
    "\n",
    "    # Tokenize the content into sentences\n",
    "    sentences1 = sent_tokenize(content_with_addition)\n",
    "    \n",
    "    \n",
    "    # Function to tokenize content into sentences\n",
    "    def tokenize_content(content):\n",
    "        return sent_tokenize(content)\n",
    "    \n",
    "    # Function to remove stop words and tokenize each sentence into words\n",
    "    def tokenize_sentences(sentences):\n",
    "        return [\n",
    "            [word for word in word_tokenize(sentence.lower()) if word.isalpha() and word not in stop_words]\n",
    "            for sentence in sentences\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    # Step 1: Calculate Term Frequency (TF) for a sentence\n",
    "    def term_frequency(doc):\n",
    "        tf = Counter(doc)\n",
    "        total_terms = len(doc)\n",
    "        return {word: count / total_terms for word, count in tf.items()}\n",
    "    \n",
    "    # Step 2: Calculate Inverse Document Frequency (IDF) for the entire corpus\n",
    "    def inverse_document_frequency(corpus):\n",
    "        N = len(corpus)\n",
    "        idf = {}\n",
    "        all_words = set(word for doc in corpus for word in doc)\n",
    "        for word in all_words:\n",
    "            containing_docs = sum(1 for doc in corpus if word in doc)\n",
    "            idf[word] = math.log(N / (1 + containing_docs))  # Add 1 to avoid division by zero\n",
    "        return idf\n",
    "    \n",
    "    # Step 3: Compute the IDF-modified cosine similarity between two sentences\n",
    "    def idf_modified_cosine_similarity(doc_x, doc_y, idf):\n",
    "        tf_x = term_frequency(doc_x)\n",
    "        tf_y = term_frequency(doc_y)\n",
    "        common_words = set(tf_x.keys()).union(set(tf_y.keys()))\n",
    "        \n",
    "        numerator = sum(tf_x.get(word, 0) * tf_y.get(word, 0) * (idf[word] ** 2) for word in common_words)\n",
    "        sum_x = sum((tf_x.get(word, 0) * idf[word]) ** 2 for word in tf_x)\n",
    "        sum_y = sum((tf_y.get(word, 0) * idf[word]) ** 2 for word in tf_y)\n",
    "        \n",
    "        denominator = math.sqrt(sum_x) * math.sqrt(sum_y)\n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        return numerator / denominator\n",
    "    \n",
    "    \n",
    "    # Step 4: Create a similarity matrix for the entire corpus\n",
    "    def create_similarity_matrix(tokenized_sentences, idf):\n",
    "        n = len(tokenized_sentences)\n",
    "        similarity_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                similarity_matrix[i][j] = idf_modified_cosine_similarity(tokenized_sentences[i], tokenized_sentences[j], idf)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    \n",
    "    def save_last_row(similarity_matrix, as_array=True):\n",
    "        last_row = similarity_matrix[-1]  # Extract the last row\n",
    "        if as_array:\n",
    "            return last_row  # Return as NumPy array\n",
    "        else:\n",
    "            return last_row.tolist()  # Return as list\n",
    "    \n",
    "    # Step 7: Print sentences corresponding to non-zero values in the last row in decreasing order\n",
    "    def save_non_zero_sentences(last_row, sentences):\n",
    "        # Create a list of (index, value) tuples for non-zero values\n",
    "        indexed_values = [(idx, value) for idx, value in enumerate(last_row) if value != 0]\n",
    "    \n",
    "        # Sort the list by value in decreasing order\n",
    "        sorted_indexed_values = sorted(indexed_values, key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "        # Save sentences corresponding to sorted indices in a string\n",
    "        updated_sentences = \"\"\n",
    "        for idx, value in sorted_indexed_values:\n",
    "            updated_sentences += f\" {sentences[idx]}\"\n",
    "    \n",
    "        return updated_sentences\n",
    "    \n",
    "    \n",
    "    def remove_last_row_and_column(matrix):\n",
    "        return matrix[:-1, :-1]\n",
    "    \n",
    "    \n",
    "    # Function to apply dropout (removing 20% of sentences) and save indices\n",
    "    def apply_dropout_exact(sentences, num_dropouts, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)  # Optional: to make the dropout reproducible\n",
    "    \n",
    "        num_sentences = len(sentences)\n",
    "        # Calculate 20% of the total number of sentences (rounded up for at least 1 sentence)\n",
    "        num_sentences_per_dropout = max(1, round(0.2 * num_sentences))\n",
    "        dropouts = []\n",
    "    \n",
    "        for _ in range(num_dropouts):\n",
    "            # Randomly sample unique sentence indices without duplicates\n",
    "            dropped_indices = random.sample(range(num_sentences), min(num_sentences_per_dropout, num_sentences))\n",
    "            \n",
    "            # Convert dropped_indices to 1-based by adding 1 to each index\n",
    "            dropped_indices = [index + 1 for index in dropped_indices]\n",
    "            \n",
    "            # Select sentences not in dropped_indices (1-based index)\n",
    "            remaining_sentences = [sentences[j] for j in range(num_sentences) if (j + 1) not in dropped_indices]\n",
    "            remaining_indices = [j + 1 for j in range(num_sentences) if (j + 1) not in dropped_indices]\n",
    "            \n",
    "            dropouts.append((remaining_sentences, remaining_indices, dropped_indices))\n",
    "    \n",
    "        return dropouts\n",
    "        \n",
    "    def modify_matrix_elements(matrix):\n",
    "        # Create a copy of the matrix to avoid changing the original one\n",
    "        modified_matrix = np.where((matrix != 0) & (matrix != None), 1, matrix)\n",
    "        return modified_matrix\n",
    "    \n",
    "    def calculate_degree_and_modify_matrix(matrix):\n",
    "        # Initialize degree count list with zeros (length of matrix)\n",
    "        degree_count = [0] * len(matrix)\n",
    "        \n",
    "        # Iterate through the matrix\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] == 1 :# Replace with 1\n",
    "                    degree_count[j] += 1  # Increase degree count for row i\n",
    "                else:\n",
    "                    matrix[i][j] = 0  # Replace with 0\n",
    "        \n",
    "        return degree_count\n",
    "    \n",
    "    def update_matrix_with_degree(matrix, degree_count):\n",
    "        # Ensure no degree count is 0 by replacing it with 1\n",
    "        adjusted_degree_count = [deg if deg != 0 else 1 for deg in degree_count]\n",
    "        \n",
    "        # Iterate through the matrix and update values based on degree\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[i])):\n",
    "                # Update matrix[i][j] with matrix[i][j] / degree[i]\n",
    "                matrix[i][j] = matrix[i][j] / adjusted_degree_count[i]\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def compute_centrality_from_matrix(similarity_matrix):\n",
    "        num_sentences = similarity_matrix.shape[0]\n",
    "        # Transpose of the similarity matrix\n",
    "        B_T = similarity_matrix.T\n",
    "        \n",
    "        # Uniform probability vector P\n",
    "        P = np.ones(num_sentences) / num_sentences\n",
    "        \n",
    "        # Compute centrality scores: B_T * P\n",
    "        centrality_scores = B_T @ P\n",
    "        \n",
    "        return centrality_scores\n",
    "    \n",
    "    def print_sentences_by_centrality(centrality_scores, sentences, threshold):\n",
    "        \n",
    "        # Filter and sort sentences based on centrality scores\n",
    "        filtered_scores = [(idx, score) for idx, score in enumerate(centrality_scores) if score > threshold]\n",
    "        sorted_scores = sorted(filtered_scores, key=lambda x: x[1], reverse=True)[0:10]\n",
    "        \n",
    "        # Print sentences with centrality scores greater than the threshold in descending order\n",
    "        print(\"Sentences with centrality scores greater than {:.2f} (sorted in descending order):\".format(threshold))\n",
    "        centrality_sentences = \"\"\n",
    "        for idx, value in sorted_scores:\n",
    "            centrality_sentences += f\" {sentences[idx]}\"\n",
    "    \n",
    "        return centrality_sentences\n",
    "\n",
    "    def remove_sparse_sentences(matrix, zero_threshold=0.01, sparse_threshold=0.8):\n",
    "        matrix = np.array(matrix)  # Ensure matrix is a NumPy array for easy manipulation\n",
    "        num_sentences = matrix.shape[0]\n",
    "        \n",
    "        # Calculate \"zero\" ratios for rows and columns\n",
    "        row_zero_ratio = np.sum(matrix < zero_threshold, axis=1) / num_sentences\n",
    "        col_zero_ratio = np.sum(matrix < zero_threshold, axis=0) / num_sentences\n",
    "        \n",
    "        # Find indices of rows and columns with more than the sparse threshold of near-zeros\n",
    "        rows_to_remove = np.where(row_zero_ratio > sparse_threshold)[0]\n",
    "        cols_to_remove = np.where(col_zero_ratio > sparse_threshold)[0]\n",
    "        \n",
    "        # Determine which sentences (rows/columns) to keep\n",
    "        rows_to_keep = [i for i in range(num_sentences) if i not in rows_to_remove and i not in cols_to_remove]\n",
    "        \n",
    "        # Filter the matrix to only keep the selected rows and columns\n",
    "        filtered_matrix = matrix[np.ix_(rows_to_keep, rows_to_keep)]\n",
    "        \n",
    "        # Map kept rows and columns back to sentence indices (1-based)\n",
    "        kept_sentences = [i + 1 for i in rows_to_keep]\n",
    "        \n",
    "        return filtered_matrix, kept_sentences\n",
    "\n",
    "    tokenized_sentences1 = tokenize_sentences(sentences1)\n",
    "    idf1 = inverse_document_frequency(tokenized_sentences1)\n",
    "    similarity_matrix1 = create_similarity_matrix(tokenized_sentences1, idf1)\n",
    "    # Example usage:\n",
    "    matrix = similarity_matrix1\n",
    "    filtered_matrix, kept_sentences = remove_sparse_sentences(matrix, zero_threshold=0.01, sparse_threshold=0.9)\n",
    "    # print(\"Filtered Matrix:\")\n",
    "    # print(filtered_matrix)\n",
    "    # print(\"Kept Sentences (Indices):\", kept_sentences)\n",
    "    def save_matrix_to_csv(filtered_matrix, filename=\"matrix.csv\"):\n",
    "        # Convert the matrix to a DataFrame\n",
    "        df = pd.DataFrame(filtered_matrix)\n",
    "        \n",
    "        # Save the DataFrame to an Excel file\n",
    "        df.to_csv(filename, index=False, header=False)\n",
    "\n",
    "    save_matrix_to_csv(filtered_matrix, \"matrix.csv\")\n",
    "    def filter_content_by_kept_sentences(content_with_addition, kept_sentences):\n",
    "        # Convert kept_sentences to 0-based indices\n",
    "        kept_indices = [i - 1 for i in kept_sentences]\n",
    "        \n",
    "        # Select only the sentences at kept indices\n",
    "        filtered_content = [content_with_addition[i] for i in kept_indices]\n",
    "        \n",
    "        # Join the filtered sentences back into a single string (if content is text)\n",
    "        new_content = \" \".join(filtered_content)\n",
    "        \n",
    "        return new_content\n",
    "    \n",
    "    # Example usage:\n",
    "    # content =\n",
    "    # kept_sentences = [1, 3, 5, 7, 10]  # Example indices of sentences to keep\n",
    "    \n",
    "    new_content = filter_content_by_kept_sentences(sentences1, kept_sentences)\n",
    "    # print(\"Filtered Content:\")\n",
    "    # print(new_content)\n",
    "    content = new_content \n",
    "    sentences = sent_tokenize(content)\n",
    "    # List to store all similarity matrices\n",
    "    similarity_matrices = []\n",
    "     # Apply dropout so that each sentence is dropped exactly once across 5 iterations\n",
    "    dropouts = apply_dropout_exact(sentences, num_dropouts = 5 , seed=None)\n",
    "    all_dropped_indices = []\n",
    "    \n",
    "    # List to store all similarity matrices\n",
    "    similarity_matrices = []\n",
    "    \n",
    "    # Process dropouts: Tokenize remaining sentences, compute similarity matrix, and keep track of indices\n",
    "    for i, (remaining_sentences, remaining_indices, dropped_indices) in enumerate(dropouts):\n",
    "        \n",
    "    \n",
    "        # Adjust dropped_indices to start from 1\n",
    "        adjusted_dropped_indices = [idx + 1 for idx in dropped_indices]\n",
    "        \n",
    "        # Save the adjusted dropped indices in the list\n",
    "        all_dropped_indices.append(adjusted_dropped_indices)\n",
    "    \n",
    "        print(f\"\\nDropout {i + 1} (Dropped sentences {adjusted_dropped_indices}):\")\n",
    "        \n",
    "        # Tokenize remaining sentences\n",
    "        tokenized_sentences = tokenize_sentences(remaining_sentences)\n",
    "        \n",
    "        # Compute IDF for the remaining sentences\n",
    "        idf = inverse_document_frequency(tokenized_sentences)\n",
    "        \n",
    "        # Create the cosine similarity matrix\n",
    "        similarity_matrix = create_similarity_matrix(tokenized_sentences, idf)\n",
    "        \n",
    "        # Save each matrix in a list\n",
    "        similarity_matrices.append(similarity_matrix)\n",
    "        \n",
    "        # Print the similarity matrix with sentence indices\n",
    "        print(f\"Remaining Indices: {remaining_indices}\")\n",
    "        # print(\"Cosine Similarity Matrix:\")\n",
    "        # print(similarity_matrix)\n",
    "    \n",
    "    # Assign names to the similarity matrices for further use\n",
    "    A1, A2, A3, A4 , A5= similarity_matrices\n",
    "    \n",
    "    # Now A1, A2, A3, A4, and A5 represent the similarity matrices for each dropout.\n",
    "    # print(A1[0:1])\n",
    "    \n",
    "    # Number of sentences (n)\n",
    "    n = len(sentences)\n",
    "    \n",
    "    # Initialize the matrix X with zeros (n x n)\n",
    "    X = np.zeros((n, n))\n",
    "    \n",
    "    # Assuming the matrices A1, A2, A3, A4, A5 are stored in the similarity_matrices list\n",
    "    # Each similarity matrix is of varying size based on remaining sentences after dropout\n",
    "    similarity_matrices = [A1, A2, A3, A4, A5]\n",
    "    print(A1)\n",
    "    # Loop through each sentence-to-sentence pair in matrix X\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            max_similarity = 0  # Initialize the maximum similarity for sentence pair (i, j)\n",
    "            \n",
    "            # Check across all dropout matrices\n",
    "            for idx, (remaining_sentences, remaining_indices, dropped_indices) in enumerate(dropouts):\n",
    "                # Check if both i+1 and j+1 are in the remaining_indices of this dropout\n",
    "                if (i+1 in remaining_indices) and (j + 1 in remaining_indices):\n",
    "                    # Find the position of i+1 and j+1 in the current matrix\n",
    "                    pos_i = remaining_indices.index(i+1 )\n",
    "                    pos_j = remaining_indices.index(j+1)\n",
    "                    \n",
    "                    # Get the similarity value from the current matrix\n",
    "                    similarity_value = similarity_matrices[idx][pos_i][pos_j]\n",
    "                    \n",
    "                    # Update max_similarity if this value is greater\n",
    "                    max_similarity = max(max_similarity, similarity_value)\n",
    "            \n",
    "            # After checking all matrices, assign the max similarity value to matrix X\n",
    "            X[i][j] = max_similarity\n",
    "\n",
    "    \n",
    "    # Print the final matrix X\n",
    "    # print(\"Final Matrix X (n x n):\")\n",
    "    print(X.T[0:1])\n",
    "\n",
    "\n",
    "    \n",
    "    last_row_array = save_last_row(X)  # Save as NumPy array\n",
    "    # print(\"Last row of the similarity matrix as array:\")\n",
    "    # Save non-zero sentences\n",
    "    result = save_non_zero_sentences(last_row_array, sentences)\n",
    "    \n",
    "    # Now the result contains the saved sentences as a string\n",
    "    print(result)  # Optional: to see the saved string\n",
    "      # Print sentences with non-zero values\n",
    "    \n",
    "    # updated_matrix = remove_last_row_and_column(X)\n",
    "    \n",
    "    updated_matrix_with_1 = modify_matrix_elements(X)\n",
    "    \n",
    "    degree_count = calculate_degree_and_modify_matrix(updated_matrix_with_1 )\n",
    "    \n",
    "    final_matrix = update_matrix_with_degree(updated_matrix_with_1, degree_count)\n",
    "    \n",
    "    centrality_scores_A = compute_centrality_from_matrix(final_matrix)\n",
    "\n",
    "    centrality_sentence = print_sentences_by_centrality(centrality_scores_A, sentences, threshold=0.001)\n",
    "    \n",
    "    print(centrality_sentence)\n",
    "    # Sample content (replace with your own content)\n",
    "    \n",
    "    \n",
    "    main = result+centrality_sentence\n",
    "\n",
    "#     reference_sentence1 = result\n",
    "#     reference_sentence2 = centrality_sentence\n",
    "#     generated_sentence = '''\n",
    "#    The average cost of heating a home can vary significantly based on several factors, including location, fuel type, and insulation. However, a general estimate for heating costs is:\n",
    "\n",
    "# - *Natural Gas*: $1.00 to $2.00 per square foot per year\n",
    "# - *Electricity*: $1.50 to $3.00 per square foot per year\n",
    "# - *Oil*: $2.00 to $3.50 per square foot per year\n",
    "# - *Propane*: $2.00 to $4.00 per square foot per year\n",
    "\n",
    "# These figures can fluctuate based on energy prices and local climate. It's advisable to consider energy efficiency and insulation quality for a more accurate estimate.\n",
    "#     '''\n",
    "#     # Initialize a ROUGE scorer\n",
    "#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "#     # Compute the ROUGE scores\n",
    "#     scores1 = scorer.score(reference_sentence1, generated_sentence)\n",
    "#     scores2 = scorer.score(reference_sentence2, generated_sentence)\n",
    "#     # Display the scores\n",
    "#     print(scores1)\n",
    "#      # Display the scores\n",
    "#     print(scores2)\n",
    "\n",
    "    \n",
    "    end_time = time.time()  # Record the end time\n",
    "    execution_time = end_time - start_time  # Calculate total execution time\n",
    "    \n",
    "    print(f\"sigmabot execution time: {execution_time:.4f} seconds\")\n",
    "        \n",
    "    def save_execution_time(model_name, execution_time, file_name='execution_times.xlsx'):\n",
    "        # Check if the file exists\n",
    "        try:\n",
    "            df = pd.read_excel(file_name)\n",
    "        except FileNotFoundError:\n",
    "            df = pd.DataFrame(columns=['Model', 'Execution Time'])  # Create new DataFrame if file doesn't exist\n",
    "    \n",
    "        # Create a new DataFrame for the new execution time\n",
    "        new_row = pd.DataFrame({'Model': [model_name], 'Execution Time': [execution_time]})\n",
    "        \n",
    "        # Concatenate the new row to the existing DataFrame\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "        # Save back to Excel\n",
    "        df.to_excel(file_name, index=False)\n",
    "    save_execution_time('Model 9', execution_time)\n",
    "\n",
    "sigmabot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed77c23-2353-490c-b2e5-b5be8c366c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf27ce8-fc1b-42df-bc27-b0134a5492e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f9093-b3a9-46ae-bec6-bbfbc8816c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
